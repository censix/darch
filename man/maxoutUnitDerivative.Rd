% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/darchUnitFunctions.R
\name{maxoutUnitDerivative}
\alias{maxoutUnitDerivative}
\title{Maxout / LWTA unit function}
\usage{
maxoutUnitDerivative(input,
  poolSize = getDarchParam("darch.layerFunction.maxout.poolSize", 2, ...),
  ...)
}
\arguments{
\item{input}{Input for the activation function.}

\item{poolSize}{The size of each maxout pool.}
}
\value{
A list with the maxout activation in the first entry and the
  derivative of the transfer function in the second entry
}
\description{
The function calculates the activation of the units and returns a list, in
which the first entry is the result through the maxout transfer function and
the second entry is the derivative of the transfer function.
}
\details{
Maxout sets the activations of all neurons but the one with the highest
activation within a pool to \code{0}. If this is used without
\link{maxoutWeightUpdate}, it becomes the local-winner-takes-all algorithm,
as the only difference between the two is that outgoing weights are shared
for maxout.
}
\seealso{
\linkS4class{DArch}

Other DArch unit functions: \code{\link{linearUnitDerivative}};
  \code{\link{rectifiedLinearUnitDerivative}};
  \code{\link{sigmoidUnitDerivative}};
  \code{\link{softmaxUnitDerivative}};
  \code{\link{tanSigmoidUnitDerivative}}
}

