% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/interface.R
\name{darch.default}
\alias{darch.default}
\title{Fit deep neural network.}
\usage{
\method{darch}{default}(x, y, layers = NULL, ..., xValid = NULL,
  yValid = NULL, scale = F, normalizeWeights = F,
  normalizeWeightsBound = 15, rbm.batchSize = 1, rbm.lastLayer = 0,
  rbm.learnRate = 1, rbm.learnRateScale = 1, rbm.weightDecay = 2e-04,
  rbm.initialMomentum = 0.5, rbm.finalMomentum = 0.9,
  rbm.momentumRampLength = 1, rbm.visibleUnitFunction = tanSigmUnitFunc,
  rbm.hiddenUnitFunction = tanSigmUnitFunc, rbm.updateFunction = rbmUpdate,
  rbm.errorFunction = mseError, rbm.numCD = 1, rbm.numEpochs = 0,
  darch = NULL, darch.batchSize = 1, darch.bootstrap = T,
  darch.genWeightFunc = generateWeightsRunif, darch.logLevel = INFO,
  darch.fineTuneFunction = backpropagation, darch.initialMomentum = 0.5,
  darch.finalMomentum = 0.9, darch.momentumRampLength = 1,
  darch.learnRate = 1, darch.learnRateScale = 1,
  darch.errorFunction = mseError, darch.dropoutInput = 0,
  darch.dropoutHidden = 0, darch.dropout.dropConnect = F,
  darch.dropout.momentMatching = 0, darch.dropout.oneMaskPerEpoch = F,
  darch.dither = F, darch.weightDecay = 0,
  darch.layerFunction = tanSigmoidUnitDerivative,
  darch.layerFunction.maxout.poolSize = NULL,
  darch.weightUpdateFunction = weightDecayWeightUpdate, darch.isClass = T,
  darch.stopErr = -Inf, darch.stopClassErr = -Inf,
  darch.stopValidErr = -Inf, darch.stopValidClassErr = -Inf,
  darch.numEpochs = 0, darch.retainData = T, darch.returnBestModel = T,
  dataSet = NULL, dataSetValid = NULL, gputools = T)
}
\arguments{
\item{x}{Input data.}

\item{y}{Target data.}

\item{layers}{Vector containing one integer for the number of neurons of each
layer. Defaults to c(\code{a}, 10, \code{b}), where \code{a} is the number
of columns in the training data and \code{b} the number of columns in the
targets.}

\item{...}{additional parameters}

\item{xValid}{Validation input data.}

\item{yValid}{Validation target data.}

\item{scale}{Logical or logical vector indicating whether or which columns to
scale.}

\item{normalizeWeights}{Logical indicating whether to normalize weights (L2
norm = 1).}

\item{normalizeWeightsBound}{Upper bound on the L2 norm of incoming weight
vectors. Used only if \code{normalizeWeights} is \code{TRUE}.}

\item{rbm.batchSize}{Pre-training batch size.}

\item{rbm.lastLayer}{\code{Numeric} indicating at which layer to stop the
pre-training. Possible values include \code{0}, meaning that all layers
are trained; positive integers, meaning to stop training after the RBM
where \code{rbm.lastLayer} forms the visible layer; negative integers,
meaning to stop the training at \code{rbm.lastLayer} RBMs from the top RBM.}

\item{rbm.learnRate}{Learning rate during pre-training.}

\item{rbm.learnRateScale}{The learn rates will be multiplied with this
value after each epoch.}

\item{rbm.weightDecay}{Pre-training weight decay. Weights will be multiplied
by (1 - \code{rbm.weightDecay}) prior to each weight update.}

\item{rbm.initialMomentum}{Initial momentum during pre-training.}

\item{rbm.finalMomentum}{Final momentum during pre-training.}

\item{rbm.momentumRampLength}{After how many epochs, relative to
\code{rbm.numEpochs}, should the momentum reach \code{rbm.finalMomentum}?
A value of 1 indicates that the \code{rbm.finalMomentum} should be reached
in the final epoch, a value of 0.5 indicates that \code{rbm.finalMomentum}
should be reached after half of the training is complete.}

\item{rbm.visibleUnitFunction}{Visible unit function during pre-training.}

\item{rbm.hiddenUnitFunction}{Hidden unit function during pre-training.}

\item{rbm.updateFunction}{Update function during pre-training.}

\item{rbm.errorFunction}{Error function during pre-training.}

\item{rbm.numCD}{Number of full steps for which contrastive divergence is
performed.}

\item{rbm.numEpochs}{Number of pre-training epochs.}

\item{darch}{Existing \code{\linkS4class{DArch}} instance for which training
is to be resumed.}

\item{darch.batchSize}{Batch size, i.e. the number of training samples that
are presented to the network before weight updates are performed (for both
pre-training and fine-tuning).}

\item{darch.bootstrap}{Logical indicating whether to use bootstrapping to
create a training and validation data set from the given data.}

\item{darch.genWeightFunc}{Function to generate the initial weights of the
DBN.}

\item{darch.logLevel}{Log level. \code{futile.logger::INFO} by default.}

\item{darch.fineTuneFunction}{Fine-tuning function.}

\item{darch.initialMomentum}{Initial momentum during fine-tuning.}

\item{darch.finalMomentum}{Final momentum during fine-tuning.}

\item{darch.momentumRampLength}{After how many epochs, relative to
the \strong{overall} number of epochs trained, should the momentum reach
\code{darch.finalMomentum}?
A value of 1 indicates that the \code{darch.finalMomentum} should be reached
in the final epoch, a value of 0.5 indicates that \code{darch.finalMomentum}
should be reached after half of the training is complete. Note that this
will lead to bumps in the momentum ramp if training is resumed with the
same parameters for \code{darch.initialMomentum} and
\code{darch.finalMomentum}. Set \code{darch.momentumRampLength} to 0 to
avoid this problem when resuming training.}

\item{darch.learnRate}{Learning rate during fine-tuning.}

\item{darch.learnRateScale}{The learning rates are multiplied by this value
after each epoch.}

\item{darch.errorFunction}{Error function during fine-tuning.}

\item{darch.dropoutInput}{Dropout rate on the network input.}

\item{darch.dropoutHidden}{Dropout rate on the hidden layers.}

\item{darch.dropout.dropConnect}{Whether to use DropConnect instead of
dropout for the hidden layers. Will use \code{darch.dropoutHidden} as the
DropConnect rate.}

\item{darch.dropout.oneMaskPerEpoch}{Whether to generate a new mask for each
batch (\code{FALSE}, default) or for each epoch (\code{TRUE}).}

\item{darch.layerFunction}{Layer function or vector of layer functions of
length \code{number of layers} - 1. Note that the first entry signifies the
layer function between layers 1 and 2, i.e. the output of layer 2. Layer 1
does not have a layer function, since the input values are used directly.}

\item{darch.layerFunction.maxout.poolSize}{Pool size for maxout units, when
using the maxout acitvation function. See \link{maxoutUnitDerivative}.}

\item{darch.weightUpdateFunction}{Weight update function or vector of weight
update functions, very similar to \code{darch.layerFunction}.}

\item{darch.isClass}{Whether classification errors should be printed
during fine-tuning. For this, network outputs are treated as binary.}

\item{darch.stopErr}{When the value of the error function is lower than or
equal to this value, training is stopped.}

\item{darch.stopClassErr}{When the classification error is lower than or
equal to this value, training is stopped (0..100).}

\item{darch.stopValidErr}{When the value of the error function on the
validation data is lower than or equal to this value, training is stopped.}

\item{darch.stopValidClassErr}{When the classification error on the
validation data is lower than or equal to this value, training is stopped
(0..100).}

\item{darch.numEpochs}{Number of epochs of fine-tuning.}

\item{darch.retainData}{Logical indicating whether to store the training
data in the \code{\linkS4class{DArch}} instance after training.}

\item{darch.returnBestModel}{Logical indicating whether to return the best
model at the end of training, instead of the last.}

\item{dataSet}{\code{\linkS4class{DataSet}} instance, passed from
darch.DataSet(), may be specified manually.}

\item{dataSetValid}{\code{\linkS4class{DataSet}} instance containing
validation data.}

\item{gputools}{Logical indicating whether to use gputools for matrix
multiplication, if available.}

\item{darch.dropout.momentumMatching}{How many iterations to perform during
moment matching for dropout inference, 0 to disable moment matching.}
}
\value{
Fitted \code{\linkS4class{DArch}} instance
}
\description{
Fit deep neural network with optional pre-training and fine-tuning.
}
\seealso{
Other darch interface functions: \code{\link{darch.DataSet}};
  \code{\link{darch.formula}}; \code{\link{darch}};
  \code{\link{predict.DArch}}, \code{\link{predict.darch}};
  \code{\link{print.DArch}}, \code{\link{print.darch}}
}

